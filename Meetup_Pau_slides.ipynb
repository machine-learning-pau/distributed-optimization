{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <a href=\"https://www.meetup.com/fr-FR/Meetup-Machine-Learning-Pau/\" ><img src=\"img/meetup_logo.png\" style=\"float:left; max-width: 120px; display: inline\" alt=\"Meetup\"/></a> \n",
    "    <a href=\"https://www.meetup.com/fr-FR/Meetup-Machine-Learning-Pau/\" ><img src=\"img/meetup_ML_pau.pdf\" style=\"max-width: 250px; display: inline\"  alt=\"Meetup Machine Learning Pau\"/></a>\n",
    "    <a href=\"http://www.helioparc.com\" ><img src=\"img/helioparc_logo.svg\" style=\"float:right; max-width: 250px; display: inline\" alt=\"Technopole Héloparc\"/></a>\n",
    "    <br>\n",
    "    <hr>\n",
    "    <h1>Optimisation distribuée avec Apache Spark</h1>\n",
    "    <hr>\n",
    "    <h3>Yann Vernaz</h3>\n",
    "    <br><br><br><br><br><br><br>\n",
    "    Crédits : LaTex, draw.io, Python, Docker, Spark, ...\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h4>Source : Nisarg Dave.</h4>\n",
    "    <img src=\"img/data_science_Nisarg_Dave.jpg\" width=\"50%\" height=\"50%\">\n",
    "</center>\n",
    "_The European Union’s new **General Data Protection Regulation**, which is slated to take effect in 2018, will create a “right to explanation\", allowing consumers to question and fight any decision made purely on an algorithmic basis that affects them._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Contexte</h1>\n",
    "    <hr>\n",
    "    <h2>Big Data & Big Models</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Big Data</h1>\n",
    "    <hr>\n",
    "</center>\n",
    "\n",
    "|     | Partenaires | Cas d'usage | Données | Volume |\n",
    "|:--- |:---:|:---:|:---:|:---:|\n",
    "| [Smart Support Center](http://www.smartsupportcenter.org) | Salesforce, HPE, LIG | Recommandation, chatbots | IoT + textes + audios | 10 Po, 100 To/jour |\n",
    "| [IKATS]() (Innovative ToolKit for Analysing Time Series) | Airbus, EDF, CS, LIG | Maintenance préventive | IoT + textes | > Po, 100 Go/vol |\n",
    "| [STREAM](http://www.legi.grenoble-inp.fr) | LEGI, LIG | Analyse de vidéos | Vidéos | 300 To/jour |\n",
    "\n",
    "- Criteo Labs (reciblage publicitaire) : 40 Po, +30 To/jour.\n",
    "- Schneider Electric R&D (IoT, maintenance préventive) : > Po.\n",
    "- Kelkoo (système de recommandation) : > Po, +10 To/jour.\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "    <h2><span style=\"color:red;\">On ne peut pas stocker les données sur une seule machine.</span></h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Big Models</h1>\n",
    "    <hr>\n",
    "    <img src=\"img/big_models2.pdf\" width=\"55%\" height=\"55%\">\n",
    "    <h4>Source : Jean-Claude Heudin, comprendre le deep learning.</h4>\n",
    "</center>\n",
    "\n",
    "- Deep Learning (CNN, RNN, )\n",
    "- Topic Models (NLP)\n",
    "- Recommandation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br>\n",
    "<center>\n",
    "    <h2><span style=\"color:red;\">Le modèle peut ne pas tenir sur une seule machine.</span></h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# source: Tesla Inc\n",
    "tesla=Image(url='img/Tesla.gif', format='gif')\n",
    "\n",
    "# source : Boston Medical Center\n",
    "lung_nodule=Image(url='img/lung_nodule.gif', format='gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/Tesla.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# source: Tesla Inc\n",
    "# Véhicule autonome\n",
    "display(tesla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/lung_nodule.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# source : Boston Medical Center\n",
    "# Nodule pulmonaire ...\n",
    "display(lung_nodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Challenges</h1>\n",
    "    <hr>\n",
    "    <h2>Les exigences dictent le choix</h2>\n",
    "    <br><br><br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Scalabilité** - Passage à l'échelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Latence** - Transition d’un monde <span style=\"color:red;\">Offline</span> vers un monde <span style=\"color:red;\">Online</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Disponibilité** : Gestion des pannes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Réduction des coûts** - cluster de PCs, cloud computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- **Intégration** : Intégration <span style=\"color:red;\">offline + online</span> pour exécuter la même business logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Systèmes Distribuées</h1>\n",
    "    <hr>\n",
    "    <h2>Hadoop ... Spark</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h2>Passage à l'échelle (Scalability)</h2>\n",
    "    <hr>\n",
    "    <img src=\"img/distributed_computing/scalability.pdf\" width=\"60%\" height=\"60%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Passer à un plus gros serveur/puissance</h1>\n",
    "    <hr>\n",
    "    <img src=\"img/distributed_computing/scale_up.pdf\" width=\"60%\" height=\"60%\">\n",
    "    <br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "    <h2><span style=\"color:red;\">Le volume des données augmente plus vite que les vitesses de traitement !</span></h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Distribuer sur plusieurs machines</h1>\n",
    "    <hr>\n",
    "    <img src=\"img/distributed_computing/scale_out.pdf\" width=\"60%\" height=\"60%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Tolérance aux pannes (Reliability)</h1>\n",
    "    <hr>\n",
    "    <img src=\"img/distributed_computing/scale_out_reliability.pdf\" width=\"60%\" height=\"60%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>PRAM Model</h1>\n",
    "    <hr>\n",
    "    <img src=\"img/distributed_computing/model_PRAM.pdf\" width=\"40%\" height=\"40%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"img/distributed_computing/cluster_PRAM.pdf\" width=\"70%\" height=\"70%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "    <h2><span style=\"color:red;\">Les communications réseau sont coûteuses !</span></h2>\n",
    "    La latence et la bande passante sont beaucoup plus rapides intra-machine qu'inter-machine.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Example : HDFS (Hadoop Distributed File System)</h1>\n",
    "    Jeffrey Dean, Sanjay Ghemawat (Google Inc.) MapReduce : simplified data processing on large clusters **OSDI** (2004).\n",
    "    <hr>\n",
    "    <img src=\"img/distributed_computing/hdfs_architecture.pdf\" width=\"100%\" height=\"100%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Paradigme de traitement : Map-Reduce</h1>\n",
    "    <hr>\n",
    "</center>\n",
    "\n",
    "Pipeline MapReduce : $\\mbox{reduce}(\\oplus)\\circ \\mbox{grp} \\circ \\mbox{map}(f)$\n",
    "\n",
    "<center><img src=\"img/distributed_computing/hadoop_concept.pdf\" width=\"100%\" height=\"100%\"></center>\n",
    "\n",
    "$\\color{red}{\\text{map}}(*2)[2, 3, 6]=[4,6,12]$\n",
    "\n",
    "$\\color{red}{\\text{grp}}[(a,2),(z,2),(ab,3),(a,4)]=[(a,[2,4]),(z,[2]),(ab,[3])]$\n",
    "\n",
    "$\\color{red}{\\text{reduce}}(+)[2,1,3]=2+1+3=6$\n",
    "\n",
    "<!-- \n",
    "Limitations de Map Reduce\n",
    "- Difficulté pour écrire du (bon) code Map-Reduce Modèle contraint.\n",
    "- Une phase de Map puis une phase de Reduce.\n",
    "- Pour les algorithmes complexes et itératifs, il faut enchaîner plusieurs phases Map-Reduce.\n",
    "- Transfert des données entre ces phases : stockage sur disque. \n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"img/spark_logo.png\" width=\"45%\" height=\"45%\"/>\n",
    "    M. Zaharia and al. Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing, **USENIX** (2012).\n",
    "    <br><br>\n",
    "    University of California, Berkeley (AMPLab).\n",
    "    <hr>\n",
    "    <h3>Généraliser Map-Reduce et unifier les traitements au sein du même moteur.</h3>\n",
    "</center>\n",
    "\n",
    "<!-- Deux ajouts sont suffisants pour exprimer les différents modèles : -->\n",
    "\n",
    "- Partager les données rapidement.\n",
    "- Construire un graphe d’exécution (Directed Acyclic Graph - DAG).\n",
    "\n",
    "Approche très efficace pour optimiser les performances et beaucoup plus simple pour les utilisateurs finaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Graphe d'exécution (DAG)</h1>\n",
    "    <hr>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/spark/DAG_1.pdf\"  width=\"50%\" height=\"50%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>RDD - Transformations</h1>\n",
    "    <hr>\n",
    "    <img src=\"img/spark/DAG_2.pdf\"  width=\"70%\" height=\"70%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>RDD - Actions</h1>\n",
    "    <hr>\n",
    "    <img src=\"img/spark/DAG_3.pdf\"  width=\"70%\" height=\"70%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Cycle de vie d'un Job Spark</h1>\n",
    "    <hr>\n",
    "    <img src=\"img/spark/Spark_job_lifetime.png\"  width=\"100%\" height=\"100%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Fonctionnement - niveau système</h1>\n",
    "    <hr>\n",
    "    <img src=\"img/spark/spark_architecture.pdf\"  width=\"70%\" height=\"70%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Écosystème</h1>\n",
    "    <hr>\n",
    "    <img src=\"img/spark/spark_MLlib.pdf\"  width=\"70%\" height=\"70%\"/>\n",
    "    <br>\n",
    "    <h3>Bibliothèque de Machine Learning</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"img/docker_logo.png\" width=\"20%\" height=\"20%\"/>\n",
    "    <h1>Déploiement et orchestration d'un cluster</h1>\n",
    "    <hr>\n",
    "</center>\n",
    "\n",
    "## [Docker Compose](http://docs.docker.com/compose)\n",
    "\n",
    "- Lancer plusieurs conteneurs en même temps. \n",
    "- Un fichier de déploiement (YAML) précise les conteneurs et leurs options.\n",
    "\n",
    "## [Docker Swarm](http://docs.docker.com/engine/swarm)\n",
    "\n",
    "- Création d'un cluster avec différentes machines (architecture Slave-Master). \n",
    "- Création d’un réseau commun que partageront tous les conteneurs. \n",
    "- Déploiement en se basant sur le fichier YAML utilisé par Docker Compose.\n",
    "\n",
    "Gestion native du **load balancing** (répartition de la charge sur les différents noeuds) et n’est pas soumis au **Single Point Of Failure** (si le master tombe il sera automatiquement remplacé par un des noeuds qui deviendra le nouveau master).\n",
    "\n",
    "Note - autre solution disponible : [Kata Containers](http://katacontainers.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Optimisation</h1>\n",
    "    <hr>\n",
    "    <h2>Lab 1 - Les algorithmes du Gradient</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h2>Example - Régression logistique</h2>\n",
    "    <hr>\n",
    "    <img src=\"img/optim/logistic_regression.pdf\" width=\"90%\" height=\"90%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ${\\bf x}_i\\in\\mathbb{R}^d$ sont les variables ou *features*.\n",
    "- $y_i\\in \\{-1 , +1 \\}$ la réponse binaire ou *label*.\n",
    "- ${\\bf w}_i\\in\\mathbb{R}^d$ les paramètres (ou poids) du modèle.\n",
    "- $n$ le nombre d'examples de notre ensemble d'apprentissage $\\mathcal{D} = \\{[{\\bf x}_i,y_i]\\}_{i=1...n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\mathbf{w}^{\\star} := \\text{arg}\\min\\limits_{\\mathbf{w}\\in \\mathbb{R}^d} \\color{red}{\\underbrace{\\frac{1}{n}\\sum_{i=1}^n \\log\\left (1+\\exp(-y_i\\mathbf{w}^T \\mathbf{x}_i) \\right )}_{\\text{ajustement aux données}}} + \\color{blue}{\\underbrace{\\frac{\\lambda}{2}\\|\\bf{w}\\|^2_2}_{\\text{régularisation}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Problème d'optimisation</h1>\n",
    "    <hr>\n",
    "</center> \n",
    "\n",
    "$$\\mathbf{w}^{\\star} := \\text{arg}\\min\\limits_{\\mathbf{w}\\in \\mathbb{R}^d} J(\\mathbf{w})\\,\\,\\,\\,\\,\\text{ avec }\\,\\,\\,\\,\\,J(\\mathbf{w})=\\color{red}{L\\left (\\mathbf{w}; \\mathbf{x},y \\right )} + \\color{blue}{R(\\mathbf{w})}$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"img/optim/loss_functions.pdf\" width=\"55%\" height=\"55%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Retour aux méthodes simples</h1>\n",
    "    <hr>\n",
    "</center>\n",
    "\n",
    "$$\\color{green}{\\underbrace{\\mathbf{w}_{k+1}}_{\\text{nouveau modèle}}} := \\color{blue}{\\underbrace{\\mathbf{w}_{k}}_{\\text{ancien modèle}}} + \\color{red}{\\underbrace{\\Delta\\mathbf{w}_{k}}_{\\text{mise-à-jour}}}$$\n",
    "\n",
    "- **Méthode itérative**\n",
    "\n",
    "- **Du premier ordre**\n",
    "\n",
    "- **Gradient Descent** \n",
    "    - Cauchy (1847)\n",
    "    - Nesterov’s optimal method (1983,2004)\n",
    "    - FISTA - Beck and Teboulle (2007)\n",
    "    \n",
    "- **Stochastic Gradient Descent** \n",
    "    - Robbins and Monro (1950)\n",
    "    - Adaptive Filtering (1960s-1990s)\n",
    "    - Back Propagation in Neural Networks (1980s) \n",
    "    - Online Learning, Stochastic Approximation (2000s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Algorithme du gradient</h1>\n",
    "    <hr>\n",
    "    $$\\mathbf{w}_{k+1} := \\mathbf{w}_{k} - \\alpha_k\\, \\nabla J(\\mathbf{w})$$\n",
    "    <br>\n",
    "    <img src=\"img/optim/gradient_descent.pdf\" width=\"80%\" height=\"80%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Example - Régression logistique</h1>\n",
    "    <hr>\n",
    "</center>\n",
    "\n",
    "$$\\mathbf{w}_{k+1} := \\mathbf{w}_{k} - \\alpha_k\\,\\left [\\color{red}{ \\frac{1}{n}\\sum_{i=1}^n \\frac{-y_i}{1+\\exp(y_i\\mathbf{w}^T \\mathbf{x}_i)}{\\mathbf x}_i } + \\color{blue}{\\lambda\\,\\mathbf{w}} \\right ]$$\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <h3>durée totale := coût par itération $\\times$ nombre d'itérations</h3>\n",
    "</center>\n",
    "\n",
    "Points importants :\n",
    "\n",
    "- Initialisation $\\mathbf{w}_{0}$, quelle stratégie d'initialisation des paramètres ?\n",
    "- Choix du pas (_learning rate_), ...\n",
    "- Le coût d'une itération est $O(nd)$. \n",
    "- Comment faire lorque $n$ et $d$ sont très grands ?\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Taux d'apprentissage (Learning Rate)</h1>\n",
    "    <hr>\n",
    "    <br>\n",
    "    <img src=\"img/optim/learning_rate.pdf\" width=\"70%\" height=\"70%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Complexité des itérations</h1>\n",
    "    <hr>\n",
    "</center>\n",
    "<br>\n",
    "Le nombre d'itérations $t(\\epsilon)$ nécessaires pour obtenir une précision $\\epsilon$.\n",
    "<br><br>\n",
    "\n",
    "$$J(\\mathbf{w}_t) - \\min J(\\mathbf{w}) \\leq \\epsilon$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/optim/algo_runtime.pdf\" width=\"80%\" height=\"80%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Batch Gradient Descent</h1>\n",
    "    <hr>\n",
    "</center>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;    0: &nbsp; Choisir une valeur initiale $\\mathbf{w}_0$ et un taux d'apprentissage $\\alpha_0>0$.<br>\n",
    "&nbsp;&nbsp;&nbsp;    1: &nbsp; <b>POUR</b> $k$ = 0, 1, 2, ... <b>FAIRE</b><br>\n",
    "&nbsp;&nbsp;&nbsp;    2: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Calculer le gradient sur l'ensemble des données : $s_k = \\frac{1}{n}\\sum_{i=1}^n\\nabla J_i(\\mathbf{w}_{k})$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    3: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Mise-à-jour des poids : $\\mathbf{w}_{k+1} = \\mathbf{w}_{k} - \\alpha_k\\, s_k$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    4: &nbsp;  <b>FIN POUR</b>\n",
    "\n",
    "|     |  fonction non régulière | fonction régulière |\n",
    "|:--- |:------ |:------ |\n",
    "| non-fortement convexe | $$O\\left (\\frac{1}{\\epsilon^2} \\right )$$| $$O\\left ( \\frac{1}{\\sqrt{\\epsilon}} \\right)$$ |\n",
    "|$\\lambda$-fortement convexe | $$O\\left (\\frac{1}{\\lambda\\epsilon} \\right )$$  | $$O\\left (\\frac{1}{\\sqrt{\\lambda}} \\log\\frac{1}{\\epsilon} \\right )$$  |\n",
    "\n",
    "<br><br>\n",
    "<center>\n",
    "    <h3>coût total := coût par itération $O(nd)$ $\\times$ nombre d'itérations</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Stochastic Gradient Descent</h1>\n",
    "    <hr>\n",
    "</center>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;    0: &nbsp; Choisir une valeur initiale $\\mathbf{w}_0$ et un taux d'apprentissage $\\alpha_0>0$.<br>\n",
    "&nbsp;&nbsp;&nbsp;    1: &nbsp; <b>POUR</b> $k$ = 0, 1, 2, ... <b>FAIRE</b><br>\n",
    "&nbsp;&nbsp;&nbsp;    2: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Choisir aléatoirement un exemple $(\\mathbf{x}_i,y_i)$ parmi l'ensemble d'apprentissage. <br>\n",
    "&nbsp;&nbsp;&nbsp;    3: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Calculer le gradient avec l'example $(\\mathbf{x}_i,y_i)$ : $s_k$ = $\\nabla J_i(\\mathbf{w}_{k})$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    4: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Mise-à-jour des poids : $\\mathbf{w}_{k+1} = \\mathbf{w}_{k} - \\alpha_k\\, s_k$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    5: &nbsp;  <b>FIN POUR</b>\n",
    "\n",
    "|     | fonction non régulière  | fonction régulière  |\n",
    "|:--- |:------ |:------ |\n",
    "| non-fortement convexe | $$O\\left (\\frac{1}{\\epsilon^2} \\right )$$ | $$O\\left (\\frac{1}{\\epsilon^2} \\right )$$ |\n",
    "|$\\lambda$-fortement convexe | $$O\\left (\\frac{1}{\\lambda\\epsilon} \\right )$$  | $$O\\left ( \\frac{1}{\\lambda\\epsilon} \\right )$$  |\n",
    "\n",
    "<br><br>\n",
    "<center>\n",
    "    <h3>coût total := coût par itération $O(d)$ $\\times$ nombre d'itérations</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Mini-Batch SGD</h1>\n",
    "    <hr>\n",
    "</center>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;    0: &nbsp; Choisir une valeur initiale $\\mathbf{w}_0$ et un taux d'apprentissage $\\alpha_0>0$.<br>\n",
    "&nbsp;&nbsp;&nbsp;    1: &nbsp; <b>POUR</b> $k$ = 0, 1, 2, ... <b>FAIRE</b><br>\n",
    "&nbsp;&nbsp;&nbsp;    2: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Choisir aléatoirement un échantillon des données de taille $m\\ll n$, qu'on notera $\\mathcal{D}_m$. <br>\n",
    "&nbsp;&nbsp;&nbsp;    3: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Calculer le gradient avec l'échantillon  $\\mathcal{D}_m$ : $s_k$ = $\\frac{1}{m}\\sum_{i=1}^{m}\\nabla J_i(\\mathbf{w}_{k})$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    4: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Mise-à-jour des poids : $\\mathbf{w}_{k+1} = \\mathbf{w}_{k} - \\alpha_k\\, s_k$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    5: &nbsp;  <b>FIN POUR</b>\n",
    "\n",
    "- Conjugue les avantages des deux méthodes précédentes.\n",
    "    - Réduit la variance des mises-à-jour des paramètres.\n",
    "    - Convergence plus stable.\n",
    "- Utilisation d'optimisations matricielles efficaces (LAPACK, ATLAS, Intel MKL, GPUs, ...).\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <h3>coût total := coût par itération $O(md)$ $\\times$ nombre d'itérations</h3>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# dynamique des processus d'apprentissage\n",
    "# Image credit: Alec Radford\n",
    "\n",
    "# Beale's function: Due to the large initial gradient, \n",
    "# velocity based techniques shoot off and bounce around - \n",
    "# adagrad almost goes unstable for the same reason. \n",
    "# Algos that scale gradients/step sizes like adadelta and \n",
    "# RMSProp proceed more like accelerated SGD and handle \n",
    "# large gradients with more stability.\n",
    "img_contours=Image(url='img/optim/contours_evaluation_optimizers.gif', format='gif')\n",
    "\n",
    "# Long valley: Algos without scaling based on gradient information \n",
    "# really struggle to break symmetry here - SGD gets no where \n",
    "# and Nesterov Accelerated Gradient / Momentum exhibits oscillations \n",
    "# until they build up velocity in the optimization direction. \n",
    "# Algos that scale step size based on the gradient quickly break \n",
    "# symmetry and begin descent.\n",
    "img_saddle=Image(url='img/optim/saddle_point_evaluation_optimizers.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/optim/saddle_point_evaluation_optimizers.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Image credit: Alec Radford\n",
    "display(img_saddle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/optim/contours_evaluation_optimizers.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Image credit: Alec Radford\n",
    "display(img_contours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Optimisation distribuée</h1>\n",
    "    <hr>\n",
    "    <h2>Lab 2 - Distribution des algorithmes du Gradient</h2>\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- Exploit many kinds of parallelism -->\n",
    "<center>\n",
    "    <h1>Deux stratégies de parallèlisation</h1>\n",
    "    <hr>\n",
    "    <img src=\"img/distributed_computing/data_model_parallelism.pdf\" width=\"90%\" height=\"90%\">\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Batch Gradient Descent\n",
    "\n",
    "Nous supposerons que $n$ est grand (nombre des données) mais pas la dimension $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "points = sc.textFile(fileName).map(parsePoint).cache()    \n",
    "w = np.zeros(d)\n",
    "\n",
    "for k in range(numIterations):\n",
    "    w_br = sc.broadcast(w)\n",
    "    gradient = points.map(lambda p : gradJ(p, w_br.value)).reduce(add)\n",
    "    w += - learningRate * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Analyse\n",
    "\n",
    "- **Barrière de synchronisation** (boucle `for`).\n",
    "- On doit attendre que tous les `mappeurs` aient terminé leur calcul avant de passer à l'itération suivante.\n",
    "\n",
    "- Coûts de communication \n",
    "    - On évite bien les communications all-to-all.\n",
    "\n",
    "<!--\n",
    "    - `map` est fortement parallèle et demande aucune communication. \n",
    "    - `broadcast` est une communication one-to-all. \n",
    "    - `reduce` est une communication all-to-one. \n",
    "-->    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h2>Synchrone : Mini-Batch Gradient Descent</h2>\n",
    "    <hr>\n",
    "    <img src=\"img/distributed_computing/synchronous_parallel_SGD.pdf\" width=\"80%\" height=\"80%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h2>Asynchrone : Mini-Batch Gradient Descent</h2>\n",
    "     HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. Feng Niu, Benjamin Recht, Christopher Re, Stephen J. Wright - http://arxiv.org/abs/1106.5730\n",
    "    <hr>\n",
    "    <img src=\"img/distributed_computing/asynchronous_parallel_SGD.pdf\" width=\"80%\" height=\"80%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <h1>Downpour SGD</h1>\n",
    "     Jeffrey Dean, and al. Large Scale Distributed Deep Networks. Neural Information Processing Systems (NIPS), pages 1–11, 2012.\n",
    "    <br>\n",
    "    cf. [TensorFlow](http://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf)\n",
    "    <hr>\n",
    "    <img src=\"img/distributed_computing/Downpour_SGD.pdf\" width=\"80%\" height=\"80%\">\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
